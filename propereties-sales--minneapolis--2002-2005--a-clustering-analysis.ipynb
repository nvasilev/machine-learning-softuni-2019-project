{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install geopandas\n",
    "# !pip install pyproj==1.9.6\n",
    "# !pip install geos\n",
    "# !pip install https://github.com/matplotlib/basemap/archive/master.zip\n",
    "# !pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn import metrics\n",
    "\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from scipy.spatial import distance_matrix\n",
    "from datetime import datetime\n",
    "\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Property Sales 2002-2005, Minneapolis: A Clustering Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Nikolay Vasilev, [nikolay.vasilev@gmail.com](mailto:nikolay.vasilev@gmail.com)_ \\\n",
    "_Software University, Bulgaria_ \\\n",
    " _23 November 2019_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This work attempts to create a framework for clustering analysis using publicly available property sales data from Minneapolis, Minnesota, USA, for the period 2002-2005. The goal is to establish a scientific approach in the data analysis which could help investors and market players take informed decisions and also to pave the road for automated approach/framework of property sales data analysis/clustering.\n",
    "\n",
    "The clustering applied highlighted four main clusters of propeties, as the characteristics which are defining them are not purely geographical but other factors such as property type are important as well.\n",
    "\n",
    "_**Keywords**: machine learning, clustering analysis, feature selection, feature creation, de-duplication, scaling, PCA, principal component analysis, KMeans++ clustering, agglomerative clustering, density based clustering analysis, DBSCAN, elbow method, dendrogram, cluster silhouette profile, word cloud, pivot table_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now-a-days property market is hard to be imagined without the usage of information technologies and yet too few actors on the market use data-driven approach when purchasing new properties. One of the advantages of the age we are living in is the abundance of data available to us and the data-processing tools which are at our disposal which we could use to harness the insights we could find in that data.\n",
    "\n",
    "The Minneapolis Assessor's Office tracks and records every real property transaction in the city as also it makes this information public. The goal of this work is to use the means of Machine Learning clustering and to establish a standarised way of identifying what are the key factors which defines the property market structure and help real estate agents, investors and future buyers take better informed decisions.\n",
    "\n",
    "At the moment on the market there are close to none analisys on the properties sales market. This research found only two public analysis on the property data [Minneapolis Area Realtors Market Data](https://www.mplsrealtor.com/market-data/) and [Grouping Minnesota Cities Using Cluster Analysis](https://www.house.leg.state.mn.us/hrd/pubs/groupmncity.pdf). The first one uses rather vanila statistics analysis to find insights for the trend on the market, while the latter applies clustered and non-clustered analysis but rather for Minnesota cities. \n",
    "\n",
    "The rest of the document is organised in several sections as follows: _Methodology_ section briefly describes what Machine Learning tools are used for the clustering analysis of the property data and how they are applied; _Data Pre-processing_ part of this document follows all steps for data preparation and sanitisation; _Finding Optimal Number of Clusters_ is the core of this work, where various Machine Learning clustering algorithms are employed to find the best organisation of the property sales in clusters; _Clustering Results: Analysis_ attempts to categorise the clusters found in order to provide a mental map to the potential consumers of this study through the property market of the city; _Conclusions_ part of this document sumarises the findings from the analysis in the previous section; in _Future Work_ a few ideas for improvement and further development of this study are listed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The methodology followed in this document is as follows: \n",
    "1. Initial data features are initially analysed and only a subset of them is selected for the actual analysis, based on the quality of the data available for each feature as well as their relevance to the conducted analysis\n",
    "1. A standard set of pre-processing actions follow, including data de-duplication, scaling, indicator variables creation and Principal Component Analysis (PCA)\n",
    "1. Clustering analysis is applied using various algorithms, namely KMeans++ Clustering, Agglomerative Clustering and Density Based Clustering Analysis (DBSCAN) with the accompanying visual methods for clustering verification such as Elbow Method, dendrogram diagrams and cluster silhouette profiles.\n",
    "1. After the identificaiton of the most optimal number of clusters for the dataset, characterisation of these clusters is attempted by using: (1) geographical distribution of the properties in each cluster, as well as (2) statistical labeling of the most frequent property features found in each cluster and (3) pivot plot analysis of the property prices for each cluster and their relation to several other properties such as ward, neighbourhood and property type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "\n",
    "The Minneapolis Assessor's Office records of real property transaction in the city is public information which has the following attributes:\n",
    "\n",
    "* **CRV** - Certificate of Real Estate Value number. A unique identifier given to every certificate of real estate value\n",
    "* **PIN** - the 13-digit tax ID that uniquely identifies a taxable parcel\n",
    "* **Sale_Date** - the date of the sale\n",
    "* **InRTOStudy** - yes or no depending on whether the sale was used for the Minnesota Department of Revenue Sales Ratio Study. Only sales that are considered fair market value sales are included\n",
    "* **Neighborhood** - the name of the neighborhood the property is in\n",
    "* **Ward** - the number of the ward that the property is in\n",
    "* **Address** - the street address of the property\n",
    "* **Grantee** - the buyer name\n",
    "* **Grantor** - the seller name\n",
    "* **Adjusted_Sale_Price** - the sale price after accounting for seller-paid adjustments\n",
    "* **Gross_Sale_Price** - the unadjusted sale price\n",
    "* **Downpayment** - the dollar value of the initial downpayment \n",
    "* **X** - the X coordinate of the parcel centroid of the sold property \n",
    "* **Y** - the Y coordinate of the parcel centroid of the sold property \n",
    "The coordinates are expressed in the NAD 83 Hennepin County HARN coordinate system (WKID: 103734)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load only the `.geo` file from all available files as it contains the majority of features we need for our analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# property_sales = gpd.read_file('./data/property-sales-2002-2005-1-1.geo')\n",
    "property_sales = gpd.read_file('https://query.data.world/s/fnwqogypfnsbz5m6nau7v4bwbbrfyi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming Dataset Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the column names are not following any specific naming convention and as we are using Python-based tools for the analysis, we are renaming them to be compliant with the Python naming convention (lowercase words separated by underscores) as well to increase the readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales.columns = ['object_id', 'crv', 'pin', 'sale_date', 'in_rto_study', 'neighborhood','ward', 'prop_type', 'address', 'house_number', 'house_number_2', 'prefix_direction', 'street_name', 'street_type', 'suffix_direction', 'grantee', 'grantor', 'adjusted_sale_price', 'gross_sale_price', 'sale_date_2', 'downpayment', 'x', 'y', 'geometry']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Duplicate Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a brief exploration we observe that there are samples in the dataset which are semantically are identical with others. For the purpose we use `cvr` which is a unique identifier given to every certificate of real estate value as well as `pin`, which is a 13-digit tax ID that uniquely identifies a taxable parcel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales.crv.duplicated().tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could see that few of the last records in the datasets have duplicates, so we list them in order to examine one of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A concrete example of a duplicate sample is a property sale with `crv` equal to `798354`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales[property_sales.crv == 798354.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the property sale is the same, but there are four records for it, because there are two sellers (`grantor`s) and two buyers (`grantee`s) in the purchase.\n",
    "\n",
    "First we will sort the properties by `crv` and then we will de-duplicate by both `crv` and `pin`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales_deduped = property_sales\n",
    "property_sales_deduped.sort_values(by=['crv'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales_deduped.drop_duplicates(subset = ['crv'], keep = False, inplace = True) \n",
    "property_sales_deduped.drop_duplicates(subset = ['pin'], keep = False, inplace = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could observe that there are no duplicated records by none of the unique features of the dataset - `crv` and `pin`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales_deduped.crv.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales_deduped.pin.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales_deduped.sort_values(by=['object_id'], inplace = True)\n",
    "property_sales = property_sales_deduped.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the size of the data set now has reduced drasticaly to `33%` of the initial size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next sections we go through each of the available features and analyse whether its data is in a sufficiently good shape for the follow-up clustering.\n",
    "\n",
    "For convenience, we group the available features in six groups during the analysis, each of which correspond to the sections below:\n",
    "* identifiers/category\n",
    "* address\n",
    "* sale date\n",
    "* sale participants\n",
    "* sale price\n",
    "* geo-related"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing Idenfier/Category Related Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `object_id`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`object_id` values are unique, therefore they are not influencing the clustering results, hence they would not be included in the clustering analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(property_sales.object_id.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `crv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`crv` (or Certificate of Real Estate Value number, a unique identifier given to every certificate of real estate value) is formally of a numeric type, but as its values are all unique, this makes it a category variable, which if later is converted into a list of indicator (dummy) variables it will increase the dimensionality of the dataset disproportionally to the number of samples available, thus reducing the precision. For that reason this feature, similarly to the `object_id` will be excluded from the analysis.\n",
    "A few checks below prove our reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(property_sales.crv.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `pin`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to `crv` and `object_id`, `pin` is an identifier, therefore we are excluding it it as a feasible feature for clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(property_sales.pin.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `in_rto_study`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(property_sales.in_rto_study.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales[property_sales.in_rto_study.isna()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ward_value_counts = property_sales.in_rto_study.value_counts(sort = False).plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this feature is boolean in its nature from puerly technical perspective it is suitable to be used in the analysis. \n",
    "\n",
    "Please note that from purely domain-related standpoint, it does not seem to influence the property sales, but as the we are not enough property sales domain-versed, we would leave this feature during the clustering with the idea that if it is not important it would be ignored after the PCA step anyway.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `ward`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales[property_sales.ward.isna()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(property_sales.ward.unique()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "ward_value_counts = property_sales.ward.value_counts(sort = False).plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a category variable with fairly small number of categories (just `13`), so we are going to leave it during the clustering. If it makes the model computation difficult or incorrect, then we will exclude it at a later step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing Address Related Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the sections below we will review whether the address related features from the input dataset are suitable for the clustering analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `address`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales[property_sales.address.isna()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(property_sales.address.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales.address.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feature seems to be a combination of `house_number`, `street_name`, `street_type` and post code features. As it is hard to be used for clustering purposes, it will be excluded from the follow-up analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `house_number` and `house_number_2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviewing `house_number` and `house_number_2` columns we could see that both columns are irrelevant to our study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The available values for `house_number` are only about `16%`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales.house_number.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(property_sales.house_number.unique())/len(property_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly `house_number_2` contains values which do not seem relevant to our analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales.house_number_2.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "As a conclusion, both features will not be included into the clustering analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `street_name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales[property_sales.street_name.isna()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(property_sales.street_name.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(140,20))\n",
    "property_sales.street_name.value_counts(sort = True).plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial idea during analysis of the input dataset was to preserve the `street_name` feature, as it seems that some streets are more popular than others, i.e. from domain point of view streets are important, but when we create indicator variables the number of dataset features is increasing several times, i.e. from `114` to `479`, which considering the small size of the dataset (`60906` before de-duplication and `20550` afterwards) is not a viable solution as it will decrease the precision of the clustering. \n",
    "\n",
    "That is why, `street_name` would be removed from the list of features to be used for clustering. When there is more input data (e.g. samples number larger than at least `100000`), then we could reconsider our decision and include `street_name` as anoter feature to our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `neighbourhood`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales[property_sales.neighborhood.isna()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len( property_sales.neighborhood.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " property_sales.neighborhood.value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(140,20))\n",
    "property_sales.neighborhood.value_counts(sort=True).plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`neighbourhood` is a category variable and although it has fairly high number of unique values (which later on would result in high number of indicator variables), it is important for the clustering from domain point of view, so we will not remove that feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `prop_type`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales[property_sales.prop_type.isna()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(property_sales.prop_type.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales.prop_type.value_counts(sort=False).plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the magnitude of teh residential properties sold is highly disproportionate to the rest of the property types on the market we will preserve this feauture as it seems important. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `prefix_direction`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales[property_sales.prefix_direction.isna()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(property_sales.prefix_direction.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales.prefix_direction.value_counts(sort=False).plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, most of the values for this feature are missing, so it will not be taken into account for our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `street_type`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales[property_sales.street_type.isna()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(property_sales.street_type.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales.street_type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales[property_sales['street_type'] == 'NULL'].shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales.street_type.value_counts(sort=False).plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the `NULL` values are negligibly small, after the initial iterations of our analysis we came to the conclusion that the impact of this feature to the outcome of our clustering is minimal, therefore we are removing it for further use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `suffix_direction`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales.suffix_direction.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales[property_sales['suffix_direction'] == 'NULL'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(property_sales[property_sales['suffix_direction'] == 'NULL'])/len(property_sales) # 5% of the data is NULL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales.suffix_direction.value_counts(sort=False).plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing values for this categorisation variable are `26%`, which means that if we exclude these samples from the dataset, it would have too high (negative) impact on the clustering (as there would not be enough number of data samples but the number of features would be too high, i.e. due to the \"curse of dimensionality\" we could introduce too much noise in the output model). \n",
    "\n",
    "Also, on a later step probably the missing `suffix_direction` values could be deduced/populated based on the geo-related information. Also due to time restriction, this will be left as a future work. \n",
    "\n",
    "In the meantime, this column will be removed from the modeling process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing Sale Date Related Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `sale_date` and `sale_date_2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that `sale_date` and `sale_date_2` have identical values, hence we dismiss the second copy (`sale_date_2`) of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales[property_sales['sale_date'] != property_sales['sale_date_2']].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to check whether converting `sale_date` to a `datetime` type would be of any use for our analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales['sale_date'] = property_sales['sale_date'].apply(pd.to_datetime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exploring `year`, `month`, `day` features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating three new properties from the date - `year`, `month` and `day`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales['year'] = property_sales.sale_date.dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales.year.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(property_sales.year, bins = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does not look like the sales have major changes throughout the years, hence we would not create a new feature for year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a `month` feature for potential use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales['month'] = property_sales['sale_date'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales.month.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(property_sales.month, bins = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a `day` feature for potential use during the clustering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales['day'] = property_sales['sale_date'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(property_sales.day, bins=31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that `day` and `month` might be useful for the clustering, although there is high probability that they will introduce too many indicator variables, which might have an impact on the precision of the clustering (due to insufficient sample data). At first, we would exclude these features from the clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does not look like we will need `sale_date` and `year` properties, so we are going to remove them from the feature set as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing Sale Participants Related Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the property market jargon in USA the seller of the property is called grantor and the person who pruchases the property - grantee."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `grantee`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales.grantee.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales[property_sales.grantee.isna()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(property_sales.grantee.unique())/len(property_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that `91%` of the grantees are different people, which means that the values in that column are virtually unique, hence we will exclude this column from the clustering analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `grantor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales.grantor.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales[property_sales.grantor.isna()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(property_sales.grantor.unique())/len(property_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to the grantees, grantors seem to be `84%` unique, hence we will exclude this feature from the clustering as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing Sale Price Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `gross_sale_price`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales.gross_sale_price.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales[property_sales.gross_sale_price.isna()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(property_sales.gross_sale_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales.gross_sale_price = property_sales.gross_sale_price.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(property_sales[property_sales.gross_sale_price == 0].gross_sale_price)/len(property_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales.gross_sale_price.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales.gross_sale_price.value_counts(sort=True) #.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of unavailable values for `gross_sale_price` is negligible (~`0.5%`), therefore filled in the missing values with `-1` in order to rectify the validity of the values in the column. It is true that we introduce bias in that way, but for now we consider it acceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `adjusted_sale_price`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales.adjusted_sale_price.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales[property_sales.adjusted_sale_price.isna()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(property_sales[property_sales.adjusted_sale_price < 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales.adjusted_sale_price = property_sales.adjusted_sale_price.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales.adjusted_sale_price.value_counts(sort=False)#.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales[property_sales.adjusted_sale_price < 0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of unavailable values for `adjusted_sale_price` is negligible (~`0.01%`), therefore filled in the missing values with `-1` in order to rectify the validity of the values in the column but there are a few values which are negative, which might mean that we cannot rely on this feature for the clustering phase, therefore we will remove this feature from the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `downpayment`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales.downpayment.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(property_sales[property_sales.downpayment == 'NULL'])/len(property_sales) # 5% of the data is NULL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As `61%` of the `downpayment` values are missing, we remove this feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing Geography Related Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `x` and `y`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per dataset features description:\n",
    "\n",
    "> _X ... and... Y ... coordinates are expressed in the NAD 83 Hennepin County HARN coordinate system (WKID: 103734)_\n",
    "\n",
    "we are not interested at the moment in these features, so we are going to remove them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `geometry`, `lat` and `lon`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales[property_sales.geometry.isna()].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As `geometry` is of type `Point`, it does seem to be an unhashable type, which would become an obstacle later on during the creation of indicator variables (dummies). That is why we create two numeric columns `lat` and `lon` respectively, which would represent lattitude and longitude:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales['lat'] = property_sales.geometry.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales['lon'] = property_sales.geometry.y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Properties Plotted on a Geo Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do a quick check whether the usage of the new latitude and longitude features are useful and whether we could gain some insights from plotting the properties on an actual geographical map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "figure = plt.figure(figsize=(15,15))\n",
    "current_axis = figure.add_subplot(111)\n",
    "\n",
    "map = Basemap(llcrnrlon=-93.356022, llcrnrlat=44.874466, urcrnrlon=-93.174347, urcrnrlat=45.063683, \\\n",
    "              resolution='i', lat_0 = 46.392410, lon_0 = -94.636230, width=100, height=200)\n",
    "map.fillcontinents(color = \"white\", lake_color = \"#7777ff\", ax = current_axis)\n",
    "\n",
    "earth_x, earth_y = map(property_sales.lat.tolist(), property_sales.lon.tolist())\n",
    "\n",
    "map.scatter(earth_x, earth_y, color=\"darkgreen\", s = 1, zorder=10)\n",
    "map.drawmapboundary(fill_color = \"#7777ff\")\n",
    "map.drawstates()\n",
    "map.drawcountries()\n",
    "map.drawrivers()\n",
    "\n",
    "plt.xlabel(\"Latitude\")\n",
    "plt.ylabel(\"Longitude\")\n",
    "plt.title(\"Properties Sold in Minnessota, 2002-2005\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first sight, visually screening the property sales plotted on a geo-map, we cannot obtain any insight on whether geo-defined clustering of the data is in place or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Redundant Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the results from the initial data analysis, we are to remove the features which do not appear useful for our clustering analysis:\n",
    "* `object_id`\n",
    "* `crv`\n",
    "* `pin`\n",
    "* `address`\n",
    "* `house_number`\n",
    "* `house_number_2`\n",
    "* `prefix_direction`\n",
    "* `street_name`\n",
    "* `street_type`\n",
    "* `suffix_direction`\n",
    "* `sale_date_2`\n",
    "* `sale_date`\n",
    "* `year`\n",
    "* `grantee`\n",
    "* `grantor`\n",
    "* `adjusted_sale_price`\n",
    "* `downpayment`\n",
    "* `x`\n",
    "* `y`\n",
    "* `geometry`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_for_removal = ['object_id', 'crv', 'pin', 'address', 'house_number', 'house_number_2', 'prefix_direction', \\\n",
    "                       'street_name', 'street_type', 'suffix_direction', 'sale_date_2', 'sale_date', 'year', 'month', \\\n",
    "                       'day', 'grantee', 'grantor', 'downpayment', 'x', 'y', 'geometry', 'adjusted_sale_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales_data = property_sales.drop(columns=columns_for_removal, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The set of initial features we end up with for clustering is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Indicator Variables/Features (Dummies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As some of the features are with category nature, e.g. `prop_type`, `ward`, `neighborhood`, based on their values we need to create indicator variables, so that we have all our features be of numerical types, thus easy to process later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales_data_dummies = pd.get_dummies(property_sales_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales_data_dummies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales_data_dummies.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe tenfold increase of the number of features, but the number is not too high. Also during the PCA step of the data pre-processing will help us discard the influence of the insignificant features for our clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the values for the various feature values vary highly, we would like to put them in the same range so that our clustering analysis does not get skewed by the various scales of the feature. For the purpose we will use `MinMaxScaler` which will place all values in the `[0,1]` range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(property_sales_data_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales_data_dummies_scaled = scaler.transform(property_sales_data_dummies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having too many features (i.e. high dimensionality), we cannot distinguish which are important ones for our analysis and which not, that is why we use Principal Component Analysis approach for linear dimensionality reduction to project it to a lower dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA().fit(property_sales_data_dummies_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales_data_dummies_scaled_transformed = pca.transform(property_sales_data_dummies_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to look into which of the variance ratios are highest in order to find new attributes, which could represent the data in the new dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the first two ratios are highest, that is why we are going to limit our reduced features to them only. \n",
    "\n",
    "Please note that both options with smaller and higher number of PCA-ed features were explored, but the optimal results are achieved with only two of them. That is why we are reducing the PCA-transformed data set to just two parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales_data_dummies_scaled_transformed_reduced = property_sales_data_dummies_scaled_transformed[:, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_sales_data_dummies_scaled_transformed_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the dataset which we would use for clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Finding Optimal Number of Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following sections we will try to cluster the data using several clustering algorithms in order to find the most optimal clustering, thus finding the most optimal internal structure of the dataset. The algorithms used are the following: `KMeans++ Clustering`, `Agglomerative Clustering` and `Density Based Clustering Analysis (DBSCAN)`.\n",
    "\n",
    "Before jumping to the actual analysis we will define some auxilliary functions which will help us visualise and work with the clustered data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxilliary Functions for Cluster and Map Drawing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(data, cluster_ids, title):\n",
    "    \"\"\"Plots clustered data in 2D format. \n",
    "\n",
    "    Parameters:\n",
    "    data (int): 2D data which will be scattered\n",
    "    cluster_ids: an array of cluster ids\n",
    "    title: Plot title\n",
    "   \"\"\"\n",
    "    plt.figure(figsize = (12,10))\n",
    "    plt.scatter(data[:, 0], data[:, 1], c = cluster_ids)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on: https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n",
    "\n",
    "def plot_kmeans_cluster_silhouette_profiles(data, clusters, clusterer):\n",
    "\n",
    "    X = data\n",
    "    cluster_labels = clusters\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    n_clusters = len(centers)\n",
    "\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors, edgecolor='k')\n",
    "\n",
    "\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "                    s=50, edgecolor='k')\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\")\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dendrogram(agglomerative_linkage, title):\n",
    "    plt.figure(figsize = (12,10))\n",
    "    dendrogram(agglomerative_linkage, leaf_rotation=0, orientation='right')\n",
    "    \n",
    "    ax = plt.gca()\n",
    "    bounds = ax.get_ybound()\n",
    "    ax.plot([1.1,1.1],bounds,'--', c='k', )\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dbscan_diagram_title(dbscan_cluster_number, dbscan_eps, dbscan_min_samples):\n",
    "    return \"Density Based (DBSCAN) Clusters: Clusters {}, for eps={}, min_samples={}\"\\\n",
    "                .format(dbscan_cluster_number, dbscan_eps, dbscan_min_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Right Number of Clusters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next sub-sections we will use several approaches to find the most optimal number of clusters for the provided dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Clustered Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "plt.scatter(property_sales_data_dummies_scaled_transformed_reduced[:, 0], \n",
    "            property_sales_data_dummies_scaled_transformed_reduced[:, 1])\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.title(\"Attempt to Visually Identify Clusters Plotting the Two PCA Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we could visually identify `4` well separated clusters, we will need to use more quantative approaches to prove that this is the most optimal number of clusters which exist for the properties sales dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KMeans++ Clustering: Elbow Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KMeans++ clustering algorithm expects number of clusters to be provided upfront. \n",
    "\n",
    "In order to select the most optimal number of clusters which properties sales dataset could be organised into, we will first calculate cluster intertias for the various configuration in terms of numbers of clusters and then apply the Elbow Method to define the most optimal clusters configuration (if the intertias/elbow function is good enough to alow it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inertias = []\n",
    "for i in range(1, 11):\n",
    "    km = KMeans(n_clusters = i)\n",
    "    km.fit(property_sales_data_dummies_scaled_transformed_reduced)\n",
    "    inertias.append(km.inertia_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Elbow Method** is is a heuristic method of interpretation and validation the consistency within clusters analysis which is designed to help finding the optimal number of clusters to which a dataset could be split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12,10))\n",
    "plt.plot(range(1, 11), inertias, marker = \"o\")\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"Inertia\")\n",
    "plt.title(\"KMeans++ Clustering: Elbow Method\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we could see the shape of the graph resembles elbow, which would mean that our data is well separated and allows for this method to be used. \n",
    "\n",
    "We could identify **`4`** as **the optimal number of clusters** as on the left hand side of the \"point `4`\" the inertias function decreases quicker than on the right side of the point, which means that increasing the number of clusters to more than 4, we would unnecessarily break the data to smaller clusters, without actually gaining any benefit. In fact, the distances between the clusters will not be that big and we ultimately will create unnecessarily high number of cluster.\n",
    "\n",
    "\n",
    "As this method is often ambiguous and not very reliable, despite its good indication in our case, to confirm/disprove its output we will apply a couple of more approaches for determining the optimal number of clusters for our dataset, namely the Silhouette method for KMeans++ Clustering and Dendrogram Diagram for Agglomerative Clustering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KMeans++ Clustering: Cluster Silhouette Profies Method "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach which could be used for finding the optimal number of clusters for a given dataset using the KMeans++ Clustering approach is the Silhouette Profiles, where such number of clusters is sought for which the distances of the points in the clusters to the centroid of the cluster are fairly equal, as well as the size of the clusters is ideally the same.\n",
    "\n",
    "We will analyse the silhouette profiles of the various number of clusters in order to prove that `4` seems to be the best choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for number_of_clusters in [3, 4, 5]:\n",
    "    kmeans_clusterer = KMeans(n_clusters = number_of_clusters)\n",
    "    kmeans_clusterer.fit(property_sales_data_dummies_scaled_transformed_reduced)\n",
    "    kmeans_clusters = kmeans_clusterer.predict(property_sales_data_dummies_scaled_transformed_reduced)\n",
    "    plot_kmeans_cluster_silhouette_profiles(property_sales_data_dummies_scaled_transformed_reduced, \\\n",
    "                                        kmeans_clusters, kmeans_clusterer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could see that the most optimal data distribution of the data in clusters is when the number of clusters is `4`. In the case of `3` and `5` the clusters are not well shaped in the sense that there are points in the cluster which are closer to the centroids than others. In the case of `4` clusters, although the size of the clusters is not the same, the centroids are in a relatively equal distance from each point in the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should note that KMeans++ works well but only if the data is well organised in clearly separated clusters. In our dataset it seems that we have well defined clusters which do not overlap which would mean that we could be fairly confident in using KMeans++ as a clustering approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agglomerative Clustering: Dendrogram Diagram Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of assessing the most optimal number of clusters for a dataset is to use the agglomerative clustering approach and more specifically to use its **dendrogram** which similarly to the Elbow Method is based on the interias of the clusters. This approach is used to find out what is the number of clusters based on the larger distances between the points on the dendrogram.\n",
    "\n",
    "We will use the default implementation of agglomerative linkage (with two clusters) provided by `scipy` to plot the dendrogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_agglomerative_linkage = linkage(property_sales_data_dummies_scaled_transformed_reduced, \\\n",
    "                                    method = \"complete\", metric = \"euclidean\")\n",
    "\n",
    "plot_dendrogram(default_agglomerative_linkage, \"Agglomerative Clustering Dendrogram of Sold Properties\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach of drawing the dendrogram of the property sales dataset is to use `seaborn` out-of-the-box functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.clustermap(property_sales_data_dummies_scaled_transformed_reduced, \\\n",
    "                   method=\"complete\", col_cluster=False,\\\n",
    "                   cmap=\"Paired\", robust = True, figsize=(12, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both cases we could see that the longest distances between clusters are achieved for cluster distances between `0.8` and `1.5`. If we draw a line perpendicular to the clustering lines, the number of intersection points gives us the optimal number of clusters the dataset could be split when using agglomerative clustering, which is again `4`.\n",
    "\n",
    "As shortly would be shown, the dataset has well shaped clusters, so all clustering algorithms used identify the same clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSAN Clustering: Visual Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately Density Based Spatial Clustering for Applications with Noise (DBSCAN) does not have any specific method for finding the number of clusters, so we will generate several clustering options changing the radius $\\epsilon$ of the vicinity and the number of noise points in order to visually identify the best number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dbscan_eps in [0.1, 0.3, 0.5, 0.7]:\n",
    "    for dbscan_min_samples in [5, 50, 200, 1000, 10000]:\n",
    "        \n",
    "        dbscan = DBSCAN(eps = dbscan_eps, min_samples = dbscan_min_samples)\\\n",
    "                            .fit(property_sales_data_dummies_scaled_transformed_reduced)\n",
    "        \n",
    "        dbscan_clusters = dbscan.labels_\n",
    "\n",
    "        dbscan_cluster_number = len(pd.Series(dbscan_clusters).unique().tolist())\n",
    "\n",
    "        plot_clusters(property_sales_data_dummies_scaled_transformed_reduced, dbscan_clusters, \n",
    "              create_dbscan_diagram_title(dbscan_cluster_number, dbscan_eps, dbscan_min_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we could see, for too small (`0.1`) or too large (i.e. `0.5` or larger) values of $\\epsilon$ the clustering is sub-optimal. For the rest of the cases, no matter what range the parameters of the DBSCAN algorithms are, it always identifies `4` clusters for our dataset.\n",
    "\n",
    "We should note that opposite to the KMeans++ Clustering algorithm, DBSCAN behaves well with data which is not shaped in spherical/well-shaped clusters, so this gives us additional confidence that the optimal number of clusters for our dataset is `4`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Clustering Results: Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have identified that the most optimal number of clusters for our dataset is `4` we would like to analyse what are the corresponding characteristics of the sold properties in each of the clusters.\n",
    "\n",
    "For the purpose, we are going to attempt analysing the data in each of the clusters from various standpoints, namely:\n",
    "\n",
    "**Geographical Location**\n",
    "This means that we will plot the clustered properties sales using their geographical coordinates to attempt understand visually whether geographical location of the properties is a significant factor in the clustering\n",
    "\n",
    "**Word Cloud** \n",
    "As the clustering could be difined as a categorisation without category labels, we will try to generate word cloud for each cluster with the features of the sold properties which are statistically significant\n",
    "\n",
    "**Pivot Plots**\n",
    "Ultimately we will approach the clustered data using the pivot tables for the highly expressed features in order to find any rules of the way how the clusters are organised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxilliary Functions for Clustering Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for Non-Visual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustered_samples(data, clusters):\n",
    "    clustered_data = data\n",
    "    clustered_data['cluster'] = clusters\n",
    "    return clustered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cluster_name(cluster, category):\n",
    "    size = len(cluster[cluster.cluster == category])\n",
    "    ratio = (size / len(cluster)) * 100\n",
    "    title = \"Cluster {} [size: {}, ratio: {:.2f}%]\".format(category, size, ratio)\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cluster_info(cluster):\n",
    "    categories = pd.Series(cluster.cluster).unique()\n",
    "    for category in categories:\n",
    "        print(create_cluster_name(cluster, category))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for Geo-Plotting Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clustered_properties_by_geo_location(data, title):\n",
    "    \"\"\"Plots clustered data using geo coordinates (without plotting the map itself). \n",
    "    The assumptions is that the number of records in the `property_sales_data` must match those in \n",
    "    `predicted_clusters`.\n",
    "    \n",
    "    Parameters:\n",
    "    property_sales_data: Dataset which contains geo coordinates of sold properties\n",
    "    predicted_clusters: The cluster type of a property\n",
    "    title: Plot title\n",
    "    \"\"\"\n",
    "    plt.figure(figsize = (12, 10))\n",
    "    sns.scatterplot(data.lat, data.lon, hue = data.cluster, \n",
    "                palette = sns.color_palette('hls', np.unique(data.cluster).shape[0]))\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clustered_properties_by_geo_location_in_separate_plot(data, feature_x_axis, feature_y_axis):\n",
    "    sns.lmplot(feature_x_axis, feature_y_axis, data=data, hue=\"cluster\", fit_reg = False, \n",
    "               col = 'cluster', col_wrap=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for Statistical Labeling Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_clustered_samples_to_text(clustered_data):\n",
    "    x = clustered_data.to_string(header=False,\n",
    "                                 index=False,\n",
    "                                 index_names=False,\n",
    "                                 formatters={\n",
    "                                     \"ward\": lambda x: \"ward_{}\".format(x),\n",
    "                                     \"neighborhood\": lambda x: x.replace(\" \", \"\"),\n",
    "                                     \"prop_type\": lambda x: x.replace(\" \", \"\"),\n",
    "                                     \"gross_sale_price\": lambda x: \"${:,.0f}grs_price\".format(x),\n",
    "                                     \"position\" : lambda point : \"({:.2f},{:.2f})\".format(point.x, point.y),\n",
    "                                     # ignoring features below:\n",
    "                                     \"in_rto_study\": lambda x: \"\",\n",
    "                                     \"cluster\" : lambda x: \"\",\n",
    "                                     \"category\": lambda x: \"\",\n",
    "                                     \"lat\" : lambda x : \"\",\n",
    "                                     \"lon\": lambda x: \"\"\n",
    "                                 }).split('\\n')\n",
    "    lines = [' '.join(ele.split()) for ele in x]\n",
    "    text = \" \".join(line for line in lines)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wordcloud(text):\n",
    "    stopwords = set(STOPWORDS)\n",
    "    stopwords.update(['hbhd', 'ward', 'lat', 'lon', 'POINT'])\n",
    "    wordcloud = WordCloud(stopwords=stopwords, background_color=\"white\", \\\n",
    "                          collocations = False, max_font_size=40,\n",
    "                          max_words=30).generate(text)\n",
    "    # print(wordcloud.words_)\n",
    "    return wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wordcloud(wordcloud, title):\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title, fontsize=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wordcloud_for_clusters(data):   \n",
    "    clusters = pd.Series(data.cluster).unique()\n",
    "    for cluster in clusters:\n",
    "        cluster_title = create_cluster_name(data, cluster)\n",
    "        text = convert_clustered_samples_to_text(data)\n",
    "        wordcloud = create_wordcloud(text)\n",
    "        plot_wordcloud(wordcloud, cluster_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_geolocation_and_wordcloud_for_clusters(data, title):  \n",
    "    clusters = pd.Series(data.cluster).unique()\n",
    "    clusters_number = len(clusters)\n",
    "    diagrams_number = 2\n",
    "\n",
    "    fig = plt.figure(figsize=(30,45))\n",
    "    plot_index = 1\n",
    "    plt.title(title, fontsize=25, fontweight='bold', pad=20)\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    for cluster_number in range(1,clusters_number+1):\n",
    "        cluster_index = cluster_number - 1\n",
    "        clustered_samples = data[data.cluster == cluster_index]\n",
    "        cluster = data[data.cluster == cluster_index].cluster\n",
    "        \n",
    "        cluster_title = create_cluster_name(data, cluster_index)\n",
    "        \n",
    "        axisId1 = clusters_number*100 + diagrams_number*10 + plot_index\n",
    "        plot_index = plot_index + 1\n",
    "        ax1 = fig.add_subplot(axisId1)\n",
    "  \n",
    "        colour = cm.nipy_spectral(float(cluster_index) / clusters_number)\n",
    "        sns.scatterplot(clustered_samples.lat, clustered_samples.lon, hue = cluster, \n",
    "                        ax=ax1, facecolor= colour, edgecolor=colour, alpha=0.7)\n",
    "        ax1.set_title(cluster_title, fontsize=14, fontweight='bold')\n",
    "        \n",
    "        axisId2 = clusters_number*100 + diagrams_number*10 + plot_index\n",
    "        plot_index = plot_index + 1\n",
    "        \n",
    "        ax2 = fig.add_subplot(axisId2)\n",
    "        \n",
    "        # print(\"Cluster: {}\".format(cluster_index))\n",
    "        text = convert_clustered_samples_to_text(clustered_samples)\n",
    "        wordcloud = create_wordcloud(text)\n",
    "        ax2.imshow(wordcloud, interpolation='bilinear')\n",
    "        ax2.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for Pivot Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot_plot(clustered_data, index_feature, x_label, fig_size_x, fig_size_y):\n",
    "    sns.set(rc={'figure.figsize':(fig_size_x,fig_size_y)})\n",
    "    pd.pivot_table(clustered_data, \n",
    "                   index = [index_feature], \n",
    "                   columns= 'cluster', \n",
    "                   values=['gross_sale_price'],\n",
    "                   fill_value= 0,\n",
    "                   aggfunc=np.sum)\\\n",
    "        .plot(kind='bar')\n",
    "\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel('Gross Sale Price')\n",
    "    plt.title('Pivot Plot: Cluster Gross Price Comparison by '+ x_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_sized_pivot_plot(clustered_data, index_feature, x_label):\n",
    "    fig_size_x = 20\n",
    "    fig_size_y = 10\n",
    "    pivot_plot(clustered_data, index_feature, x_label, fig_size_x, fig_size_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot_plot_aggregated_data(data, aggregation):\n",
    "    sns.set(rc={'figure.figsize':(10,5)})\n",
    "    pivot_table = pd.pivot_table(data, \n",
    "                                index = ['cluster'], \n",
    "                               columns= 'cluster', \n",
    "                               values=['total'],\n",
    "                               fill_value= 0)\\\n",
    "                    .plot(kind='bar')\n",
    "\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel('{} Gross Sale Price ($)'.format(aggregation))\n",
    "    plt.title('Pivot Plot: Cluster Gross Price Comparison by {} Gross Price'.format(aggregation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will generate clusters using each of the algorithms explored above, so that we could analyse their output in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KMeans++ Clustering: 4 Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_clusterer_4_clusters = KMeans(n_clusters = 4)\n",
    "kmeans_clusterer_4_clusters.fit(property_sales_data_dummies_scaled_transformed_reduced)\n",
    "kmeans_4_clusters = kmeans_clusterer_4_clusters.predict(property_sales_data_dummies_scaled_transformed_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agglomerative Clusterring: 4 Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agglomerative_clusterer_4_clusters = AgglomerativeClustering(n_clusters=4)\n",
    "agglomerative_clusterer_4_clusters.fit(property_sales_data_dummies_scaled_transformed_reduced)\n",
    "agglomerative_4_clusters = agglomerative_clusterer_4_clusters.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN Clustering: 4 Clusters (eps = 0.3, min_samples = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_clusterer_4_clusters = DBSCAN(eps=0.3, min_samples=200).fit(property_sales_data_dummies_scaled_transformed_reduced)\n",
    "dbscan_4_clusters = dbscan_clusterer_4_clusters.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enhance Sales Data With Cluster Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before continuing our analysis let us add the found cluster numbers/labels to our `property_sales_data` to help the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_property_sales_data_kmeans = clustered_samples(property_sales_data, kmeans_4_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_property_sales_data_aggl = clustered_samples(property_sales_data, agglomerative_4_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_property_sales_data_dbscan = clustered_samples(property_sales_data, dbscan_4_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Results: Analysis by Cluster Sizes and Cluster Identity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let review what are the sizes of the clusters generated by each of the algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_cluster_info(clustered_property_sales_data_kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_cluster_info(clustered_property_sales_data_aggl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_cluster_info(clustered_property_sales_data_dbscan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, perhaps due to the structure of the data (i.e. well defined groups without overlapping), the sizes of the clusters produced by each of the algorithms are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array_equal(clustered_property_sales_data_kmeans['cluster'].tolist(), \n",
    "               clustered_property_sales_data_aggl['cluster'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array_equal(clustered_property_sales_data_kmeans['cluster'].tolist(), \n",
    "               clustered_property_sales_data_dbscan['cluster'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could conclude that the **clusters identified by each of the algorithms** are **identical**, hence we could do the further analysis using the results generated by just one of the algorithms.\n",
    "\n",
    "For the rest of the sections, unless stated otherwise, we will use the KMeans++ clustering results for generalising our conclusions for the properties sales data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Results: Analysis by Geographical Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clustered_properties_by_geo_location(clustered_property_sales_data_kmeans, \n",
    "                                          \"KMeans++ Clustering of Sold Properties: 4 Clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently overlaying all clusters on the same map does not give us very clear idea how strongly the geographical distribution of the properties influences the clusters structure. There is partial clarity on the presence of properties from cluster `0` and `1` in the central part of the city, while properties from clusters `2` and `3` are rather in the outscirts, but it is hard to read.\n",
    "\n",
    "We will geographically plot the clusters each on its own and try to eventually find any relation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_clustered_properties_by_geo_location_in_separate_plot(clustered_property_sales_data_kmeans, 'lat', 'lon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could see that **geography** has **partial impact on the cluster structure**. \n",
    "\n",
    "Our observations are as follows:\n",
    "* Cluster `0` and `1` are:\n",
    "    * similar in shape\n",
    "    * have clear presence in the **central parts of the city**\n",
    "* Cluster `2` and `3` \n",
    "    * are similar in shape\n",
    "    * have clear presence in the **periphery of the city**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Results: Analysis by Statistical Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One approach to analyse the clustered data is to convert its feautres to text and analyse statistically which are the most frequent words/strings met in the text for each cluster. \n",
    "\n",
    "The auxilliary function we have defined above `convert_clustered_samples_to_text(...)` does exactly that, as the features which are not defining the properties in a cluster such as `in_rto_study`, `latitude` and `longitude` are excluded, and those which are used are the following:\n",
    "* `ward`\n",
    "* `neighborhood`\n",
    "* `prop_type`\n",
    "* `gross_sale_price`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_geolocation_and_wordcloud_for_clusters(clustered_property_sales_data_kmeans, \n",
    "                                            \"Properties Sales Clusters: n_clusters=4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have to convert word clouds above into a tabular format, the data would look like the one below (please note that the order in the sells is important - top values are most frequent while the bottom ones - less):\n",
    "```\n",
    "                            Statistically Defined Cluster Labels\n",
    "\n",
    "    Feature       Cluster: 0           Cluster 1         Cluster 2    Cluster 3  \n",
    "\n",
    " Gross Price    $0               $0                     $0           $0          \n",
    "                $900,000         $900,000               $900,000     $500,000    \n",
    "                $500,000         $500,000               $500,000     $900,000    \n",
    "\n",
    " Property Type  Condominium      Condominium            Residential  Residential \n",
    "                Double Bungalow  Apartment                                       \n",
    "                Apartment        Double Bungalow                                 \n",
    "                Townhouse        Commercial                                      \n",
    "                Commercial       VacantLand                                      \n",
    "                Triplex          Residential                                     \n",
    "                                 Apartment Condominium                           \n",
    "\n",
    " Ward           Ward 3           Ward 3                 Ward 4       Ward 4      \n",
    "                Ward 7           Ward 7                 Ward 12      Ward 5      \n",
    "                Ward 10          Ward 10                Ward 13      Ward 12     \n",
    "                Ward 1           Ward 5                 Ward 11      Ward 13     \n",
    "                Ward 5           Ward 9                 Ward 1       Ward 1      \n",
    "                Ward 9           Ward 6                 Ward 8       Ward 9      \n",
    "                Ward 8           Ward 13                Ward 5       Ward 8      \n",
    "                Ward 2           Ward 8                 Ward 2       Ward 11     \n",
    "                Ward 13          Ward 12                Ward 9       Ward 2      \n",
    "                Ward 4           Ward 1                 Ward 7       Ward 3      \n",
    "                Ward 6           Ward 2                              Ward 7      \n",
    "                Ward 12                                              Ward 10     \n",
    "                Ward 11                                                          \n",
    "\n",
    " Neighbourhood  NORTHLOOP        NORTHLOOP              FULTON       WILLARD     \n",
    "                DOWNTOWNWEST     DOWNTOWNWEST           STANDISH     HAY         \n",
    "                LORINGPARK       NICOLLETIS             VICTORY      JORDAN      \n",
    "                WHITTIER         EASTBANK               WILLARD      FOLWELL     \n",
    "                ELLIOTPARK       WHITTIER               HAY          HOWE        \n",
    "                POWDERHORNPARK   STEVENSSQ              JORDAN       HAWTHORNE   \n",
    "                HOLLAND          LORINGHGTS             HOWE         VICTORY     \n",
    "                LOWRYHILLEAST    ELLIOTPARK             LINDENHILLS  LIND        \n",
    "                                 DOWNTOWNEAST           FOLWELL      BOHANON     \n",
    "                                                        WAITEPARK    WEBBER      \n",
    "                                                        LIND         CAMDEN      \n",
    "                                                        BOHANON      MCKINLEY    \n",
    "                                                        ARMATAGE     CENTRAL     \n",
    "                                                        WEBBER       LINDENHILLS \n",
    "                                                        CAMDEN                   \n",
    "                                                        LYNNHURST                \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gross Price\n",
    "We could conclude that due to the small changes in each of the **property sale prices**, this feature **could not be used in statistical labeling** in its **raw form**. It needs to be pre-processed and replaced by price ranges first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Property Type\n",
    "There is a clear distinction between the property types of the different clusters:\n",
    "* **clusters `0` and `1`** are having **higher class properties**:\n",
    "    * `Condominium`s\n",
    "    * `Double Bungalow`s and `Apartment`s\n",
    "    * as well as `Compertial` properties\n",
    "* **clusters `2` and `3`** are having **`Residential`** type properties only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ward-wise, we could identify the clear rule which we observed in the geo-location analysis, that:\n",
    "* **cluster `0` and `1` overlap** in their core wards, i.e.:\n",
    "    * the majority of the properties belonging to these clusters share the same wards:\n",
    "        * `Ward 3`\n",
    "        * `Ward 7` \n",
    "        * `Ward 10`\n",
    "        * `Ward 1` and `Ward 5`\n",
    "* **cluster `2` and `3` overlap** in their core wards, i.e.:\n",
    "    * the majority of the properties belonging to these clusters share the same wards:\n",
    "        * `Ward 4`\n",
    "        * `Ward 5` \n",
    "        * `Ward 12`\n",
    "        * `Ward 13`\n",
    "        * `Ward 1`\n",
    "        * `Ward 8` and `Ward 9`\n",
    "* clusters `0` and `1` **do not overlap** with clusters `2` and `3` in their core wards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neighbourhood\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding neighbourhoods, the overlap is not fully replicated from the wards:\n",
    "* **cluster `0` and `1` overlap** in their core neighbo:\n",
    "    * the majority of the properties belonging to these clusters share the same wards:\n",
    "        * `NORTHLOOP`\n",
    "        * `DOWNTOWNWEST` \n",
    "* **cluster `2` and `3` overlap** in their core wards, i.e.:\n",
    "    * the majority of the properties belonging to these clusters share the same wards:\n",
    "        * `Ward 4`\n",
    "        * `Ward 5` \n",
    "        * `Ward 12`\n",
    "        * `Ward 13`\n",
    "        * `Ward 1`\n",
    "        * `Ward 8` and `Ward 9`\n",
    "* clusters `0` and `1` **do not overlap** with clusters `2` and `3` in their core wards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Results: Analysis by Pivot Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this sub-section we will try to find some rules which define the various clusters as for a tool we will use pivot plots to visualise the property gross pricess in relation to several features.\n",
    "\n",
    "The features for which we would like to analyse the property gross price per cluster are subset of the properties considered in the labeled statistical analysis:\n",
    "* Property Type\n",
    "* Ward\n",
    "* Neighbourhood\n",
    "\n",
    "Please note that the mean is used for rendering the prices in order to compact the graphics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_sized_pivot_plot(clustered_property_sales_data_kmeans, 'prop_type', 'Property Type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the total gross sale price of properties per property type, there is clear distinction of:\n",
    "* cluster `0` consists of high-end properties - `Comercial`, `Apartment`, `Condominium` and `Double Bungalow`\n",
    "* cluster `1` is characterised by properties mostly of type `Condominium`, `Apartment`, `Comercial`, `Vacant Land - Industrial` and less of types `Double Bungalow` and `Industrial` properties\n",
    "* cluster `2` is highly represented by properties of type `Residential`\n",
    "* cluster `3` is as well `Residential` properties but of lower price and quantity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_sized_pivot_plot(clustered_property_sales_data_kmeans, 'ward', 'Ward')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_size_x = 140\n",
    "fig_size_y = 20\n",
    "\n",
    "pivot_plot(clustered_property_sales_data_kmeans, 'neighborhood', 'Neighbourhood', fig_size_x, fig_size_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The price band of clusters `0` and `1` in general higher than the one of clusters `3` and `4`, as there are some outliers in each the clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gross_price_sum = clustered_property_sales_data_kmeans.groupby('cluster')\\\n",
    "                    .gross_sale_price.sum()\\\n",
    "                    .reset_index(name='total')\n",
    "\n",
    "pivot_plot_aggregated_data(gross_price_sum, 'Total')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the amount of the sold residential properties in cluster `2` its share of the market is high (`49.89%`) hence the total amount (in terms of gross price) is highest, followed by cluster `1` and `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gross_price_mean = clustered_property_sales_data_kmeans.groupby('cluster')\\\n",
    "                    .gross_sale_price.mean()\\\n",
    "                    .reset_index(name='total')\n",
    "\n",
    "pivot_plot_aggregated_data(gross_price_mean, 'Mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The highest end properties in terms of gross sale price are in cluster `0` and `1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can conclude that the properties sales market in Minneapolis for the period of 2002-2005 is shaped by two main factors - location and property types as the four main types of properties could be organised in four main groups/clusters:\n",
    "* **Comercial and High-end Living Properties** \n",
    "    * Cluster `0`\n",
    "    * 20.46% of the market\n",
    "    * Consisting of Comercial Spaces, Apartments, Condominiums and Double Bungalows\n",
    "    * located mostly in the city centre and the outskirts of the city:\n",
    "        * Wards: 3, 7, 10, 1\n",
    "        * Neighbourhoods: Northloop, Downtown-West, Loring Park, Whittier\n",
    "* **High-end Living Properties with some Comercial Properties and Industrial Land** \n",
    "    * Cluster `1`\n",
    "    * 20.76% market share\n",
    "    * Consisting of Condominiums, Apartments, Comercial Properties, Industrial Vacant Land and Industrial Properties\n",
    "    * Similarly to the previous group the properties in this one are located mostly in the city centre and the outskirts of the city:\n",
    "        * Wards: 3, 7, 10, 5\n",
    "        * Neighbourhoods: Northloop, Downtown-West, Nicolletis, Eeast Bank \n",
    "* **Residential Properties** \n",
    "    * Cluster `2`\n",
    "    * 49.89% - highest market share\n",
    "    * Consisting mostly of Residential Properties\n",
    "    * The location of these properties are mostly out of the central part of the city:\n",
    "        * Wards: 4, 12, 13, 11, 1\n",
    "        * Neighbourhoods: Fulton, Standish, Victory, Willard, Hay\n",
    "* **Low-end Residential Properties** \n",
    "    * Cluster `3`\n",
    "    * 8.89% - highest market share\n",
    "    * Consisting mostly of low-end Residential Properties       \n",
    "    * The location of these properties are mostly out of the central part of the city:        \n",
    "        * Wards: 4, 5, 12, 13, 1\n",
    "        * Neighbourhoods: Willard, Hay, Jordan, Folwell, Howe\n",
    "\n",
    "Future buyers and investors could use this data to make informed decisions and navigate on the property market of Minneapolis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the course exam some deficiencies of this work were pointed out which need to be addressed accordinlgy before developing it further:\n",
    "* Reducing the number of samples from `~70,000` to `~20,000` does not influence the clustering, as the duplicated samples would just project to the same cluster points later on.\n",
    "* PCA is not really needed for the clustering and the original set of features seem to be sufficient\n",
    "* Even if PCA is used, currently it uses just around `30%` of the new parameters which matter (ideally the aim should be to use at least `90%`). Please see `pca.explained_variance_ratio_` above\n",
    "* KMeans Clustering is not really applicable to this use case as the clusters discovered do not seem to be spherical, while KMeans need spherically distributed data\n",
    "* Looking at the silhouette profiles it is clearly visible that the clusters are not very similar in terms of shape and size hence, further exploration would need to be done, e.g. \n",
    "    * need to be verified how the silhouette profiles of 11 or 12 clusters look like\n",
    "\n",
    "Once the main points above are addressed, this analysis could be extended further, as the main directions work could be done are as follows:\n",
    "* Generalising the analysis for newer data sets (e.g. 2010-2013)\n",
    "* Pre-processing the gross price of properties into price bands in order to be leverage statistically labeling as well as to add it to as characteristics of the property clusters/groups\n",
    "* Automating the data analysis and pre-processing using Pandas Pipelines\n",
    "* Unifying the colours of the clusters across the Jupiter Notebook\n",
    "* Render ward and neighbourhood names on the map/cluster plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Property Sales 2002-2005, Minneapolis, Minnesota, USA](https://data.world/minneapolismn/342cfcca2eae40838adf5602299dfcd4-0)\n",
    "2. [Open Minneapolis, Property Sales 2002-2005](http://opendata.minneapolismn.gov/datasets/342cfcca2eae40838adf5602299dfcd4_0)\n",
    "3. [Minneapolis City Council Wards](https://www.arcgis.com/home/item.html?id=0e9be43abc834e85bc6ad003d9358c76)\n",
    "4. [City of Minneapolis Communities & Neighbourhoods Map](http://www.minneapolismn.gov/www/groups/public/@cped/documents/maps/convert_273414.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
